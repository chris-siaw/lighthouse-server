name: Run Lighthouse and Store Results for Multiple Pages

on:
  schedule:
    - cron: '0 12 * * *'  # Runs daily at 12:00 UTC
  workflow_dispatch:      # Allows manual execution

jobs:
  lighthouse:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v2
        with:
          node-version: '18'

      - name: Install Lighthouse CLI
        run: npm install -g lighthouse

      - name: Install @vercel/blob Dependency
        run: npm install @vercel/blob

      - name: Install PostgreSQL Client
        run: sudo apt-get install -y postgresql-client

      - name: Install jq (JSON Parser)
        run: sudo apt-get install -y jq

      - name: Install Chromium Browser
        run: sudo apt-get install -y chromium-browser

      - name: Loop Through URLs, Upload Reports & Insert into DB
        env:
          VERCEL_BLOB_TOKEN: ${{ secrets.VERCEL_BLOB_TOKEN }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          # Fetch URLs from GitHub Secrets (comma-separated)
          TARGET_URLS="${{ secrets.TARGET_URL }}"
          IFS=',' read -r -a URL_ARRAY <<< "$TARGET_URL"
          
          # Loop through each URL
          for url in "${URL_ARRAY[@]}"; do
            echo "Running Lighthouse for $url"
            TEMP_DIR=$(mktemp -d)
            echo "Temporary directory created: $TEMP_DIR"
            
            # Generate Lighthouse reports (JSON & HTML)
            lighthouse "$url" --output=json --quiet --chrome-flags="--no-sandbox --headless --disable-dev-shm-usage" --output-path="$TEMP_DIR/report.json"
            lighthouse "$url" --output=html --quiet --chrome-flags="--no-sandbox --headless --disable-dev-shm-usage" --output-path="$TEMP_DIR/report.html"
            
            # Verify JSON report exists; if not, skip this URL
            if [ ! -f "$TEMP_DIR/report.json" ]; then
              echo "Error: Lighthouse did not generate report.json for $url. Skipping..."
              rm -rf "$TEMP_DIR"
              continue
            fi
            
            # Extract scores using jq
            PERFORMANCE=$(jq -r '.categories.performance.score * 100' "$TEMP_DIR/report.json")
            ACCESSIBILITY=$(jq -r '.categories.accessibility.score * 100' "$TEMP_DIR/report.json")
            BEST_PRACTICES=$(jq -r '.categories."best-practices".score * 100' "$TEMP_DIR/report.json")
            SEO=$(jq -r '.categories.seo.score * 100' "$TEMP_DIR/report.json")
            echo "Performance: $PERFORMANCE, Accessibility: $ACCESSIBILITY, Best Practices: $BEST_PRACTICES, SEO: $SEO"
            
            # Create a safe filename and move the HTML report to the working directory
            REPORT_FILENAME="lighthouse_$(echo $url | sed 's/[^a-zA-Z0-9]/_/g').html"
            mv "$TEMP_DIR/report.html" "$REPORT_FILENAME"
            echo "Moved report file to: $REPORT_FILENAME"
            ls -al "$REPORT_FILENAME"
            
            # Upload the report using the Node.js script and capture its output
            UPLOAD_OUTPUT=$(node scripts/uploadToVercelBlob.js "$REPORT_FILENAME")
            echo "Upload script output: $UPLOAD_OUTPUT"
            
            # Extract the public URL from the output (assumes it contains a URL starting with 'https://')
            REPORT_URL=$(echo "$UPLOAD_OUTPUT" | grep -o 'https://[^ ]*')
            echo "Extracted report URL: $REPORT_URL"
            
            if [ -z "$REPORT_URL" ]; then
                echo "Error: No report URL extracted for $url"
            else
                # Insert the run data into the database.
                # Column order: url, performance_score, accessibility_score, best_practices_score, seo_score, timestamp, report_url
                SQL="INSERT INTO lighthouse_tests (url, performance_score, accessibility_score, best_practices_score, seo_score, timestamp, report_url)
                VALUES ('$url', $PERFORMANCE, $ACCESSIBILITY, $BEST_PRACTICES, $SEO, NOW(), '$REPORT_URL');"
                echo "$SQL"
                echo "$SQL" | psql "$DATABASE_URL"
            fi
            
            # Clean up the temporary directory
            rm -rf "$TEMP_DIR"
            echo "Temporary directory cleaned up: $TEMP_DIR"
          done

      - name: Debug - Final Check
        run: |
          echo "Current directory:"
          pwd
          echo "Listing all files in repo (recursively):"
          ls -R
